{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:blue\">Target: </span>\n",
    "## <span style=\"color:blue\">Combine Xarray and Dask for the postprocessing of climate model output</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xarray\n",
    "<img src=\"images/dataset-diagram-logo.png\" \n",
    "     align=\"right\"\n",
    "     width=\"60%\"\n",
    "     alt=\"Xarray Dataset\">\n",
    "*  **NumPy-like multidimensional arrays**\n",
    "*  **Enable labels**\n",
    "*  **Preserve attributes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**what does a xarray look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"/depot/huberm/data/kong/archive/CORDEX-EAS/PhaseII/historical/1h/tas/tas_EAS-22_MOHC-HadGEM2-ES_historical_r1i1p1_GERICS-REMO2015_v1_1hr_2000-2000.nc\"\n",
    "tas=xr.open_dataset(file)\n",
    "#xarray.dataset: can contain mulitple data variables\n",
    "tas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xarray.DataArray: only one data variable: it's good to do calculations with xarray.DataArray\n",
    "tas.tas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why label is useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**indexing by label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas.tas.sel(time=\"2000-01-01T01:00:00\").max([\"rlat\",\"rlon\"]) #spatial maximum of one time-step\n",
    "#tas.sel(time=slice(\"2000-01-01\",\"2000-01-02\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**you can still do indexing by index with isel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tas.isel(time=[0])\n",
    "tas.isel(time=slice(None,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply operations over dimensions by name:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas.tas.mean('time') #time average (annual avearage in this case) distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**array broadcasting based on dimension names; never need to reshape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas.tas-tas.tas*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**split-apply-combine paradigm with groupby: easier to calculate monthly, seasonal, daily, zonal, or longitudinal statistics!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas.tas.groupby('time.month').mean() #daily average pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "tas.tas.mean('time').plot(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xarray documentation\n",
    "http://xarray.pydata.org/en/stable/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask\n",
    "<img src=\"images/dask-horizontal.svg\" \n",
    "     align=\"right\"\n",
    "     width=\"40%\"\n",
    "     alt=\"Dask arrays are blocked numpy arrays\">\n",
    "\n",
    "Provides multi-core and distributed ***parallel*** execution on ***larger-than-memory*** datasets.\n",
    "\n",
    "*  **High level collections:**  Dask Array mimics NumPy arrays but can operate in parallel on datasets that don't fit into memory.\n",
    "   \n",
    "*  **Low Level schedulers:** Task schedulers that execute task graphs in parallel. These execution engines power the high-level collections mentioned above but can also power custom, user-defined workloads.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Array\n",
    "<img src=\"images/dask-array-black-text.svg\" \n",
    "     align=\"right\"\n",
    "     width=\"60%\"\n",
    "     alt=\"Dask arrays are blocked numpy arrays\">\n",
    "Dask arrays coordinate many Numpy arrays, arranged into chunks within a grid.  They support a large subset of the Numpy API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**what does a dask array look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='/depot/huberm/data/kong/archive/CORDEX-EAS/PhaseII/historical/1h/hurs/hurs_EAS-22_MOHC-HadGEM2-ES_historical_r1i1p1_GERICS-REMO2015_v1_1hr_2000-2000.nc'\n",
    "hurs=xr.open_dataset(file, chunks={\"time\": 1000})\n",
    "hurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**we are actually viewing a dask array within xarray structure; *.data* will let us access dask array directly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hurs.hurs.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**let's compare the performance of doing calculations on xarray and dask array (within xarry structure)**\n",
    "\n",
    "*we expect dask array to be faster thanks to parallel computing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "tas.tas.mean('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "hurs.hurs.mean('time').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**why it's even slower?**\n",
    "**let's try both of them again!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "tas.tas.mean('rlat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "hurs.hurs.mean('rlat').compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "both xarray and dask array are faster at the second time. That's because both xarray and dask array are lazy loading. Xarray won't load array values into memory until you perform actual computation (you can manipulate, slice and subset xarray without loading into memory). In fact Dask is even lazier; it won't load anything until we explicitely call .compute()\n",
    "\n",
    "\n",
    "if memeory is not a problem (for example when you are using a cluster), you can load your data first (*dataset.load()* for xarry; *dask.array.persist()* for Dask) which will distribute your data into cluster memory. Afterwards, computation would be faster!\n",
    "\n",
    "Dask performs worse at the first try, since we need to load the data chunk by chunk\n",
    "\n",
    "However, dask didn't show better performance even at the second try which probably indicate that the extra overhead of using dask makes it not suitable for this *small* problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The power of xarray and dask can be well combined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas=tas.chunk({\"time\": 500}) #convert tas to dask array\n",
    "tas_by_season = tas.tas.groupby('time.season').mean('time')\n",
    "tas_range = abs(tas_by_season.sel(season='JJA')- tas_by_season.sel(season='DJF'))\n",
    "%time tas_range.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delayed\n",
    "parallelize custom algorithms using the simpler dask.delayed interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def inc(x):\n",
    "    time.sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def double(x):\n",
    "    time.sleep(1)\n",
    "    return x * 2\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(1)\n",
    "    return x + y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "data = [1, 2, 3]\n",
    "output = []\n",
    "for x in data:\n",
    "    a = inc(x)\n",
    "    b = double(x)\n",
    "    c = add(a, b)\n",
    "    output.append(c)\n",
    "total = sum(output)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = [1, 2, 3]\n",
    "output = []\n",
    "for x in data:\n",
    "    a = dask.delayed(inc)(x)\n",
    "    b = dask.delayed(double)(x)\n",
    "    c = dask.delayed(add)(a, b)\n",
    "    output.append(c)\n",
    "total = dask.delayed(sum)(output)\n",
    "total.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total.visualize() #dask is drawing this graph when the actual computation is delayed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dask.delayed best practice:\n",
    "https://docs.dask.org/en/latest/delayed-best-practices.html\n",
    "\n",
    "### Don’t call dask.delayed on Dask Arrays\n",
    "When you place a Dask array into a delayed call, that function will receive the NumPy equivalent. Beware that if your array is large, then this might crash your workers.\n",
    "### Use *da.map_blocks*, *xr.apply_ufunc*, or *xr.map_blocks* instead to apply customized function to dask arrays chunk by chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viscosity(tas):\n",
    "    omega = (tas / 97 - 2.9) / 0.4 * (-0.034) + 1.048\n",
    "    viscosity = 0.0000026693 * ((28.97 * tas)**0.5) / (3.617**2 * omega)\n",
    "    return viscosity\n",
    "\n",
    "v1=da.map_blocks(viscosity,tas.tas.data)\n",
    "v2=xr.apply_ufunc(viscosity,tas.tas,dask=\"parallelized\",output_dtypes=[float])\n",
    "v3=xr.map_blocks(viscosity,tas.tas)\n",
    "v4=viscosity(tas.tas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time v1.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time v2=v2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time v3=v3.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time v4=v4.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best practice of Dask\n",
    "https://docs.dask.org/en/latest/best-practices.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scheduler\n",
    "* **Single machine scheduler:** Default scheduler, can only be used on a single machine. If you import Dask, set up a computation, and then call compute, then you will use the single-machine scheduler by default. ***We use single-machine scheduler above by default!***\n",
    "\n",
    "\n",
    "* **Distributed scheduler:** can run on a single machine or distributed across a cluster, **should be preferred even on a single machine** (offer more features like diagnostic). To use the dask.distributed scheduler you must set up a Client\n",
    "```python\n",
    "from dask.distributed import Client\n",
    "client = Client(...) \n",
    "df.x.sum().compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client=Client()#sets up a scheduler in your local process and several processes running single-threaded Workers.\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tas=tas.chunk({\"time\": 1000})\n",
    "v=da.map_blocks(viscosity,tas.tas.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time v.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single process with multiple threads\n",
    "client = Client(processes=False)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=da.map_blocks(viscosity,tas.tas.data)\n",
    "%time v.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "single process with multiple threads works better for numeric computations like numpy who releasing GIL. But for other calculations don't release GIL, the parallel capability of multi-thread disappear! Use multi-process instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple processes each with single thread\n",
    "client = Client(n_workers=2,threads_per_worker=12) \n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=da.map_blocks(viscosity,tas.tas.data)\n",
    "%time v.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multiple processes each with single thread\n",
    "client = Client(n_workers=24,threads_per_worker=1) \n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time v.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "each process has its own memory which solve the GIL issue. But, the memory of each process may be not enough for a big chunk size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed across cluster: multiple nodes\n",
    "\n",
    "### The preferred way to run Dask on HPC systems is to use *dask-jobqueue*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "cluster = SLURMCluster(\n",
    "    queue=\"huberm\",\n",
    "    cores=24,\n",
    "    processes=1,\n",
    "    local_directory='/scratch/brown/kong97/tmp',\n",
    "    project=\"huberm\",\n",
    "    memory=\"80 GB\",\n",
    "    walltime=\"01:00:00\",\n",
    "    interface='ib0'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster.job_script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tas_rechunk2 = tas.chunk({\"time\":100})\n",
    "v_rechunk2=da.map_blocks(viscosity,tas_rechunk2.tas.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time v_rechunk2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /depot/huberm/data/kong/archive/CORDEX-EAS/PhaseII/historical/1h/sfcwind/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfcwind=xr.open_mfdataset('/depot/huberm/data/kong/archive/CORDEX-EAS/PhaseII/historical/1h/sfcwind/*MOHC-HadGEM2-ES*.nc', parallel=True)\n",
    "sfcwind.sfcWind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfcwind.sfcWind.chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numba: *@njit @vectorize*\n",
    "### *@njit decorator*: compile your python code into machine code just-in-time\n",
    "\n",
    "### *@vectorize*: Using @vectorize, you write your function as operating over input scalars, rather than arrays. Numba will generate the surrounding loop (or kernel) allowing efficient iteration over the actual inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit, vectorize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x,y):\n",
    "    return x*x+np.exp(y)-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def f2(x,y):\n",
    "    return x*x+np.exp(y)-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.random.randn(1000, 1000,100)\n",
    "b=np.random.randn(1000, 1000,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f1(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f2(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.random.randn(1000, 1000,10)\n",
    "b=np.random.randn(1000, 1000,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x,y):\n",
    "    z=np.zeros((1000,1000,10))\n",
    "    for i in np.arange(0, a.shape[0]):\n",
    "        for j in np.arange(0, a.shape[1]):\n",
    "            for k in np.arange(0, a.shape[2]):\n",
    "                z[i,j,k]=x[i,j,k]+y[i,j,k]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def f4(x,y):\n",
    "    z=np.zeros((1000,1000,10))\n",
    "    for i in np.arange(0, a.shape[0]):\n",
    "        for j in np.arange(0, a.shape[1]):\n",
    "            for k in np.arange(0, a.shape[2]):\n",
    "                z[i,j,k]=x[i,j,k]+y[i,j,k]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f3(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time f4(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g1(x,y):\n",
    "    if x>y:\n",
    "        return x+y\n",
    "    else:\n",
    "        return x-y\n",
    "@vectorize(nopython=True)\n",
    "def g2(x,y):\n",
    "    if x>y:\n",
    "        return x+y\n",
    "    else:\n",
    "        return x-y\n",
    "g3=np.vectorize(g1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time g3(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time g2(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My Own Kernel)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
